# -*- coding: utf-8 -*-
"""PvNet_infer_toGeodata

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IC48rTo3m6m1cFPP5fpIwZLe6P0v9gAF

# Reset
"""

#!kill -9 -1 # reset virtual machine
# refresh the page after a minute or so...
!date; uptime

"""Confirm that TPU works"""

'''import os
import pprint
import tensorflow as tf

if 'COLAB_TPU_ADDR' not in os.environ:
  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')
else:
  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']
  print ('TPU address is', tpu_address)

  with tf.Session(tpu_address) as session:
    devices = session.list_devices()
    
  print('TPU devices:')
pprint.pprint(devices)'''

"""# Settings"""

import datetime
import os
now_str = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

# input images to run inference on
TEST_IMGS_PATH = '/content/drive/My Drive/Arbeit/IIP/PANGIS_TenneT/PvNet/datasets/Sat_Bamberg/'
#TEST_IMGS_PATH = '/content/drive/My Drive/Arbeit/IIP/PANGIS_TenneT/PvNet/datasets/Sat_Regensburg/'
#TEST_IMGS_PATH = '/content/drive/My Drive/Arbeit/IIP/PANGIS_TenneT/PvNet/datasets/Sat_Deggendorf_Auswahl_01/'
#TEST_IMGS_PATH = '/content/drive/My Drive/Arbeit/IIP/PANGIS_TenneT/PvNet/datasets/Sat_Regensburg_industrial/'
#TEST_IMGS_PATH = '/content/drive/My Drive/Arbeit/IIP/PANGIS_TenneT/PvNet/datasets/Sat_Regensburg_rail/'

# Weights file to use for detection
WEIGHTS_FOLDER = '/content/drive/My Drive/Arbeit/IIP/PANGIS_TenneT/PvNet/output/weights/'
#WEIGHTS_FILE = '20180913_233352/mask_rcnn_via_0060.h5'  # works with 4 classes
#WEIGHTS_FILE = '20190109_105814/mask_rcnn_via_0080.h5'  # works with 5 classes: "BG","PVR","FREE","FLAT","ST"
#WEIGHTS_FILE = '20190123_233419/mask_rcnn_via_0105.h5'  # val_loss=0.8743; works with 5 classes: "BG","PVR","FREE","FLAT","ST"
#WEIGHTS_FILE = '20190129_082511/mask_rcnn_via_0015.h5'  # val_loss=1.960; works with 5 classes: "BG","PVR","FREE","FLAT","ST"
#WEIGHTS_FILE = '20190129_082511/mask_rcnn_via_0040.h5'  # val_loss=1.097; works with 5 classes: "BG","PVR","FREE","FLAT","ST"
#WEIGHTS_FILE = '20190201_131254/mask_rcnn_via_0055.h5'  # val_loss=1.1173; works with 5 classes: "BG","PVR","FREE","FLAT","ST"
#WEIGHTS_FILE = '20190204_074234/mask_rcnn_via_0060.h5'  # val_loss=2.788; works with 5 classes: "BG","PVR","FREE","FLAT","ST"
#WEIGHTS_FILE = '20190206_151830/mask_rcnn_via_0020.h5'  # val_loss=1.424; works with 5 classes: "BG","PVR","FREE","FLAT","ST"
#WEIGHTS_FILE = '20190208_111617/mask_rcnn_via_0040.h5'  # val_loss=1.187; works with 5 classes: "BG","PVR","FREE","FLAT","ST"
#WEIGHTS_FILE = '20190208_111617/mask_rcnn_via_0050.h5'  # val_loss=1.311; works with 5 classes: "BG","PVR","FREE","FLAT","ST"
#WEIGHTS_FILE = '20190209_193056/mask_rcnn_via_0050.h5'  # loss=0.1172, val_loss=1.971, started from 20190208_111617/mask_rcnn_via_0050
#WEIGHTS_FILE = '20190211_134357/mask_rcnn_via_0030.h5'  # loss: 0.0912, val_loss: 2.4115, started from 20190209_193056/mask_rcnn_via_0050
#WEIGHTS_FILE = '20190213_073310/mask_rcnn_via_0010.h5'  # loss: 0.1287, val_loss: 1.6724, started from 20190211_134357/mask_rcnn_via_0030
#WEIGHTS_FILE = '20190213_204720/mask_rcnn_via_0040.h5'  # loss: 0.1203, val_loss: 2.0347, started from 20190213_073310/mask_rcnn_via_0010
#WEIGHTS_FILE = '20190215_120118/mask_rcnn_via_0040.h5'  # loss: 0.2079, val_loss: 1.6355, ResNet50, started from scratch
WEIGHTS_FILE = '20190218_165412/mask_rcnn_via_0040.h5'  # loss: 0.0939, val_loss: 2.2184, started from 20190213_204720/mask_rcnn_via_0040
WEIGHTS_PATH = os.path.join(WEIGHTS_FOLDER, WEIGHTS_FILE)
useLatestWeightsFile = True  # this will overwrite the WEIGHTS_PATH with the latest found within RESULT_WEIGHTS_DIR

# result directory to store Geodata results in
#RESULT_DIR = os.path.join('/content/drive/My Drive/Arbeit/IIP/PANGIS_TenneT/PvNet/output/Regensburg/', now_str) + '/'
#RESULT_DIR = os.path.join('/content/drive/My Drive/Arbeit/IIP/PANGIS_TenneT/PvNet/output/Sat_Deggendorf_Auswahl_01/', now_str) + '/'
RESULT_DIR = os.path.join('/content/drive/My Drive/Arbeit/IIP/PANGIS_TenneT/PvNet/output/Geodata/', now_str) + '/'

"""# Connect to Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# Dependencies & imports"""

# Tensorflow & models
import os
os.chdir('/content')
!apt-get install -y -qq protobuf-compiler python-pil python-lxml
!pip install lxml
!git clone --quiet https://github.com/tensorflow/models.git
os.chdir('/content/models/research')
!protoc object_detection/protos/*.proto --python_out=.
import sys
sys.path.append('/content/models/research/slim')

# test if it works
#%run object_detection/builders/model_builder_test.py

# coco
import os
os.chdir('/content')
!pip install Cython
!pwd; ls
!git clone https://github.com/waleedka/coco
!pip install -U setuptools
!pip install -U wheel
!make install -C coco/PythonAPI

import os

# clone Mask R-CNN and checkout release 2.1 (555126e Balloon Color Splash sample). for other releases, see: https://github.com/matterport/Mask_RCNN/releases
os.chdir('/content')
!git clone https://github.com/matterport/Mask_RCNN
os.chdir('/content/Mask_RCNN')
!git checkout 555126ee899a144ceff09e90b5b2cf46c321200c

# check out Mask R-CNN, newest release
#!os.chdir('/content'); git clone https://github.com/matterport/Mask_RCNN.git

# download the coco weights file
#!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5
!pwd; ls

# reinstall Keras 2.1.6 instead of the last version (https://github.com/matterport/Mask_RCNN/issues/694)
!pip install 'keras==2.1.6' --force-reinstall

# pyshp to create shapefiles, see https://gis.stackexchange.com/questions/85448/creating-polygon-shapefile-from-list-of-x-y-coordinates-using-python
!pip install pyshp
import shapefile as shp

# OpenCV
import cv2 as cv

# Shapely to create Shapefiles
!pip install pyproj
!pip install shapely

# required for coordinate projection
import pyproj    
import shapely
import shapely.ops as ops
from shapely.geometry.polygon import Polygon
from functools import partial

"""# Copy over data from GDrive

copy over test images
"""

IMAGE_DIR = '/content/test/img'  # Directory of images to run detection on
!mkdir -pv '{IMAGE_DIR}'
!ls -al '{TEST_IMGS_PATH}'
!rsync -auv --include="*/" --include="*.jpg" --exclude="*" --delete '{TEST_IMGS_PATH}' '{IMAGE_DIR}'
#!ls -al '{IMAGE_DIR}'

if useLatestWeightsFile:
  import glob
  WEIGHTS_PATH = max(glob.iglob('/content/drive/My Drive/Arbeit/IIP/PANGIS_TenneT/PvNet/output/weights/2019*/*.h5'), key=os.path.getctime)  # latest weights file

"""# Process images"""

import os
os.chdir('/content/Mask_RCNN')
sys.path.append('/content/Mask_RCNN/mrcnn')
import sys
import random
import math
import numpy as np
import skimage.io
import matplotlib
import matplotlib.pyplot as plt

# Root directory of the project
ROOT_DIR = os.path.abspath('/content/Mask_RCNN')

# Import Mask RCNN
sys.path.append(ROOT_DIR)  # To find local version of the library
#from mrcnn import utils
import utils
#import mrcnn.model as modellib
import model as modellib
#from mrcnn import visualize
import visualize
# Import COCO config
sys.path.append(os.path.join(ROOT_DIR, "samples/coco/"))  # To find local version
import coco

# %matplotlib inline 

# Directory to save logs and trained model
MODEL_DIR = os.path.join(ROOT_DIR, "logs")

class InferenceConfig(coco.CocoConfig):
    # Set batch size to 1 since we'll be running inference on
    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1
    # Don't resize imager for inferencing
#    IMAGE_RESIZE_MODE = "pad64"

config = InferenceConfig()

# PV Class names
class_names = [
        'BG', 
        'PVR', 'FREE', 'FLAT', 'ST'
    ]

config.NUM_CLASSES = len(class_names)
config.display()

# Create model object in inference mode.
model = modellib.MaskRCNN(mode="inference", model_dir=MODEL_DIR, config=config)

# Load weights
model.load_weights(WEIGHTS_PATH, by_name=True)

def find_instances_in_image(IMAGE_DIR, filename):
  image = skimage.io.imread(os.path.join(IMAGE_DIR, filename))
  width = image.shape[0]
  height = image.shape[1]
  
  # Run detection
  results = model.detect([image], verbose=1)
  r = results[0]
  
  # Visualize results (skip for faster processing)
  #visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])
  #plt.show()
  
  runTime = time.time()-imgStartTime
  print("processing took", runTime, "s")
  
  # extract coordinates from filename
  coords = filename.split('_')[1:-1]
  minLat = float(coords[0])
  minLon = float(coords[1])
  maxLat = float(coords[2])
  maxLon = float(coords[3])
  # compensate for wrong order in filename
  if(minLat < 20):  # lat=~46-55, lon=~5-16 in Germany
    temp1 = minLat
    temp2 = maxLat
    minLat = minLon
    maxLat = maxLon
    minLon = temp1
    maxLon = temp2
  lonExt = maxLon-minLon
  latExt = maxLat-minLat
  
  # Number of instances
  N = r['rois'].shape[0]
  print("found", N, "instances.")
  
  instances = []
  
  for i in range(N):  # iterate over found instances
    
    # save center of instance as single point in shapefile
    #w.point((x1+x2)/2/width*(maxLon-minLon),(y1+y2)/2/height*(maxLat-minLat))
    #w.record(class_names[r['class_ids'][i]])
    
    # save bounding box as polygon in shapefile
    #y1, x1, y2, x2 = r['rois'][i]
    #bbox_wgs84 = [[minLon+x1/width*lonExt,minLat+(1-y1/height)*latExt], [minLon+x1/width*lonExt,minLat+(1-y2/height)*latExt], 
    #              [minLon+x2/width*lonExt,minLat+(1-y2/height)*latExt], [minLon+x2/width*lonExt,minLat+(1-y1/height)*latExt],
    #              [minLon+x1/width*lonExt,minLat+(1-y1/height)*latExt]
    #             ]
    #w.poly([bbox_wgs84])
    #w.record(TYPE=class_names[r['class_ids'][i]], SCORE=r['scores'][i])
    
    # create polygon from raster (mask): https://gis.stackexchange.com/questions/187877/how-to-polygonize-raster-to-shapely-polygons    https://github.com/cocodataset/cocoapi/issues/39
    mask = r['masks'][:,:,i]
    #plt.imshow(mask)
    #plt.show()
    mask = mask.astype(np.uint8)
    _, contours, hierarchy = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
    
    if(contours):
      cnt = contours[0]
      
      # apply Ramer–Douglas–Peucker algorithm for polygon approximation
      epsilon = 0.01*cv.arcLength(cnt,True)
      approx = cv.approxPolyDP(cnt,epsilon,True)
      approxPointsList_px = approx[:,0,:]
      
      # TODO: check if instance at an edge of the image was found => need to re-create image
      delta = 3
      touched_edges_LRTB = [
          len(np.where(approxPointsList_px[:,0] < delta)[0]) > 1,    # at least 2 points on left border of image
          len(np.where(approxPointsList_px[:,0] > width-1-delta)[0]) > 1,    # at least 2 points on right border of image
          len(np.where(approxPointsList_px[:,1] < delta)[0]) > 1,    # at least 2 points on top border of image
          len(np.where(approxPointsList_px[:,1] > height-1-delta)[0]) > 1    # at least 2 points on bottom border of image
          ]
      nTouchedEdges = np.sum(touched_edges_LRTB)
      if nTouchedEdges > 0:
        print("touched_edges_LRTB:", touched_edges_LRTB)
        #visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])
        #plt.show()
        #new_im = Image.new('RGB', (width, height))
      if nTouchedEdges == 1:  # instance touches 1 edge => create composite image from 2 images
        print("need to compose new image from 2 images");
        
      elif nTouchedEdges == 2:  # instance touches 2 edges => create composite image from 3 images
        print("need to compose new image from 3 images");
        
      elif nTouchedEdges > 2:  # instance touches 3+ edges => we need to zoom out
        print("need to zoom out");
      # how to re-create image by combining tiles: https://stackoverflow.com/questions/30227466/combine-several-images-horizontally-with-python
      #new_im = Image.new('RGB', (width, height))
      #new_im.paste(im1, (x_offset,y_offset))
      #new_im.paste(im2, (x_offset,y_offset))
      #new_im.save('test.jpg')
      
      # convert to WGS84 coordinates
      approxPointsList = approxPointsList_px.astype(type('float', (float,), {}))
      for point in approxPointsList:
        point[0] = minLon + point[0] / (width-1) * lonExt
        point[1] = minLat + (height-1-point[1]) / (height-1) * latExt
      approxPointsList = np.concatenate([approxPointsList,[approxPointsList[0]]])  # add first point at the end to close polygon
      
      # calculate 'azimuth': angle between vertical axis and shorter side of rotated bounding rectangle
      rotBBox = cv.minAreaRect(approx)
      if rotBBox[1][0] < rotBBox[1][1]:  # width < height. see https://stackoverflow.com/questions/15956124/minarearect-angles-unsure-about-the-angle-returned/21427814#21427814
        az = rotBBox[2] + 90
      else:
        az = rotBBox[2] + 180
      
      # apply coordinate transformation/projection to LAEA, to calculate area of the polygon in m2  https://gis.stackexchange.com/questions/127607/area-in-km-from-polygon-of-coordinates
      if len(approxPointsList) > 3:
        geom = Polygon(approxPointsList)
        geom_aea = ops.transform(partial(pyproj.transform, pyproj.Proj(init='EPSG:4326'), pyproj.Proj(proj='laea', lat1=geom.bounds[1], lat2=geom.bounds[3])), geom)
        sz=geom_aea.area
      else:
        sz = 0
        
      instances.append([approxPointsList, approxPointsList_px, class_names[r['class_ids'][i]], r['scores'][i], az, sz, touched_edges_LRTB])
      
      # end of check for viable contours
    # end of iteration over instances
  return instances

import time

# Iterate all images from the images folder
startTime = time.time()

# create geodata dict/map: filename -> tuple(fileSize, instances), 
# and instances = [[approxPointsList, approxPointsList_px, class_name, SCORE, az, sz]]
# info on dict type: https://www.w3schools.com/python/python_dictionaries.asp
geodata =	{}
  
totalArea = 0
nInstances = 0
for root, dirs, files in os.walk(IMAGE_DIR):
  for f, filename in enumerate(files):
    imgStartTime = time.time()
    print("processing image " +str(f)+"/"+str(len(files)) + " (" +str((f+1)/len(files)*100)+ "%): <"+ filename + ">")
    #if f > 100:
    #  break
    
    instances = find_instances_in_image(IMAGE_DIR, filename)
    fileSize = os.path.getsize(os.path.join(IMAGE_DIR, filename))
    geodata[filename] = [fileSize, instances]
    nInstances += len(instances)
    for i in instances:
      totalArea += i[5]
    
  # end of iteration over files

print("finished after", (time.time()-startTime), "s.")
print("processing took on average " + str((time.time()-startTime)/len(files)) + "s per image.")
print(str(totalArea) + "m² PV area detected in total.")
print(str(nInstances) + " PV areas detected in total.")


"""# Export Geodata results"""

import glob
from shutil import copy

# Create results directory
if not os.path.exists(RESULT_DIR):
    os.makedirs(RESULT_DIR)

geoDataFileName = 'PvNet_' + (WEIGHTS_PATH.split('/')[-2:][0]+"/"+WEIGHTS_PATH.split('/')[-1:][0]).replace("/mask_rcnn_via_", "_").replace(".h5", "")

# create Shapefile (*.shp, *.dbf, *.shx)
!rm -r '/content/shp'
w = shp.Writer('/content/shp/' + geoDataFileName)
w.field("TYPE", "C") # saves type, e.g. 'pv_roof'
w.field("SCORE", 'N', decimal=4) # Score/probability
w.field("AZ", 'N', decimal=1) # angle of the minimum bounding rectangle, range [0, 180)  https://stackoverflow.com/questions/15956124/minarearect-angles-unsure-about-the-angle-returned
w.field("SZ", 'N', decimal=1) # size of the covered area [m2]
for filename, d in geodata.items():
  for i in d[1]:
    w.poly([i[0]])
    w.record(TYPE=i[2], SCORE=i[3], AZ=i[4], SZ=i[5])
w.close()

# copy over result files
for f in glob.iglob('/content/shp/*'):
  copy(f, RESULT_DIR)

# create VIA training data file (*.json)
RESULT_JSON_PATH = '/content/'+geoDataFileName+'.json'
!rm {RESULT_JSON_PATH}
with open(RESULT_JSON_PATH, "a") as jsonFile:
  # via file header
  jsonFile.write("{\"_via_settings\":{\"ui\":{\"annotation_editor_height\":10,\"annotation_editor_fontsize\":0.8,\"leftsidebar_width\":18,\"image_grid\":{\"img_height\":80,\"rshape_fill\":\"none\",\"rshape_fill_opacity\":0.3,\"rshape_stroke\":\"yellow\",\"rshape_stroke_width\":2,\"show_region_shape\":true,\"show_image_policy\":\"all\"},\"image\":{\"region_label\":\"type\",\"region_label_font\":\"10px Sans\"}},\"core\":{\"buffer_size\":18,\"filepath\":{},\"default_filepath\":\"./\"},\"project\":{\"name\":\"PV_DATA\"}},\"_via_img_metadata\":{\n") 
f = 0
for filename, d in geodata.items():
  # image header
  with open(RESULT_JSON_PATH, "a") as jsonFile:
    jsonFile.write("\n\""+filename+str(fileSize)+"\":{\"filename\":\""+filename+"\",\"size\":"+str(d[0])+",\"regions\":[")
    for i in d[1]:
      # instance/region data
      jsonFile.write("{\"shape_attributes\":{\"name\":\"polygon\""
                     +",\"all_points_x\":"+np.array2string(i[1][:,0], precision=2, separator=',', suppress_small=True)
                     +",\"all_points_y\":"+np.array2string(i[1][:,1], precision=2, separator=',', suppress_small=True)
                     +"},\"region_attributes\":{\"type\":\""+i[2]+"\"}},")
  # remove comma after last instance
  with open(RESULT_JSON_PATH, 'rb+') as jsonFile:
    if len(d[1]) > 0:
      jsonFile.seek(-1, os.SEEK_END)
      jsonFile.truncate()
  # image footer
  with open(RESULT_JSON_PATH, "a") as jsonFile:
    jsonFile.write("],\"file_attributes\":{}}"+("," if f < len(geodata.items())-1 else ""))  # suppress comma if last file
  f+=1
  # via file footer
with open(RESULT_JSON_PATH, "a") as jsonFile:
  jsonFile.write("\n\n},\"_via_attributes\":{\"region\":{\"type\":{\"type\":\"dropdown\",\"options\":{\"PVR\":\"PV system on the roof of a building\",\"FREE\":\"free standing PV system\",\"FLAT\":\"PV system on a flat roof\",\"ST\":\"Solar thermal collectors\"},\"default_options\":{\"FLAT\":true}}},\"file\":{}}}")

# copy over result file
copy(RESULT_JSON_PATH, RESULT_DIR)

# create Geopackage File (*.gpkg)    http://www.loicdutrieux.net/blog/2017/07/08/geopackage/
!pip install fiona
import fiona
from fiona.crs import from_string

schema = {'geometry': 'Polygon',
          'properties': [('class_name', 'str'),
                         ('score', 'float'),
                         ('az', 'float'),
                         ('sz', 'float')]}
crs = from_string('+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs')

# Feature builder
def feature_from_instance(instance):
    geometry = {'type': 'Polygon', 'coordinates': [[ (p[0], p[1]) for p in instance[0] ]]}  # see https://fiona.readthedocs.io/en/latest/manual.html
    feature = {'geometry': geometry,
               'properties': {'class_name': instance[2],
                              'score': instance[3].item(),
                              'az': instance[4],
                              'sz': instance[5]
                             }}
    return feature

# Open connection with gpkg file in a context manager and write features to it
RESULT_GPKG_PATH = '/content/'+geoDataFileName+'.gpkg'
!rm {RESULT_GPKG_PATH}
with fiona.open(RESULT_GPKG_PATH, 'w',
                layer='PV',
                driver='GPKG',
                schema=schema,
                crs=crs) as dst:
  for filename, d in geodata.items():
    for i in d[1]:
      dst.write(feature_from_instance(i))

# copy over result file
copy(RESULT_GPKG_PATH, RESULT_DIR)

# create GeoJSON File (*.geojson)
RESULT_GEOJSON_PATH = '/content/'+geoDataFileName+'.geojson'
!rm {RESULT_GEOJSON_PATH}
with open(RESULT_GEOJSON_PATH, "a") as geoJsonFile:
  # file header
  geoJsonFile.write("{ \"type\": \"FeatureCollection\", \"features\": [")
  
  # features
  for filename, d in geodata.items():
    #print(d)
    for i, instance in enumerate(d[1]):
      geoJsonFile.write("\n{ \"type\": \"Feature\", \"geometry\": { \"type\": \"Polygon\", \"coordinates\": ")
      geoJsonFile.write(str([[ [p[0], p[1]] for p in instance[0] ]]).replace("array(", "").replace(")", "") + "},")
      geoJsonFile.write("\"properties\": {\"class_name\": \""+str(instance[2])+"\", \"score\": \""+str(instance[3])+"\", \"az\": \""+str(instance[4])+"\", \"sz\": \""+str(instance[5])+"\"}},")
      
  # file footer
  geoJsonFile.write("\n]}")

# copy over result file
copy(RESULT_GEOJSON_PATH, RESULT_DIR)

# copy result images to GDrive
for f in glob.iglob('./*.png'):
  copy(f, RESULT_DIR)

# show whats inside the result folder
for f in os.listdir(RESULT_DIR):
  print(f + "\t|\t" + str(os.path.getsize(RESULT_DIR+"/"+f)) + " bytes\t|\tlast modified: %s" % time.ctime(os.path.getmtime(RESULT_DIR+"/"+f)))